import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

# ==========================================
# 1. Mock Components (ç’°å¢ƒã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
# ==========================================

class MockQueryEncoder(nn.Module):
    """
    Qwen-Embeddingãªã©ã®Query Encoderã‚’æ¨¡å€£ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã€‚
    å…¥åŠ›IDã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚
    """
    def __init__(self, vocab_size=1000, embedding_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.proj = nn.Linear(embedding_dim, embedding_dim)

    def forward(self, input_ids):
        # ç°¡æ˜“çš„ãªMean Pooling
        embeds = self.embedding(input_ids) 
        return self.proj(embeds.mean(dim=1)) 

class MockEnvironment:
    """
    RAGã®ç’°å¢ƒï¼ˆLLM + Ground Truthï¼‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚
    å®Ÿç’°å¢ƒã§ã¯ã“ã“ãŒã€ŒLLMã¸ã®å•ã„åˆã‚ã›ã€ã¨ã€ŒF1ã‚¹ã‚³ã‚¢è¨ˆç®—ã€ã«ãªã‚Šã¾ã™ã€‚
    """
    def __init__(self, doc_embeddings, ground_truth_indices_map):
        self.doc_embeddings = doc_embeddings
        self.gt_map = ground_truth_indices_map 

    def get_reward(self, query_id, selected_doc_indices):
        """
        å ±é…¬é–¢æ•°ã€‚
        æ­£è§£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚Œã°é«˜ã„å ±é…¬(0.9~1.0)ã‚’è¿”ã—ã¾ã™ã€‚
        """
        target_doc_idx = self.gt_map[query_id]
        if target_doc_idx in selected_doc_indices:
             return np.random.uniform(0.9, 1.0) 
        else:
             return np.random.uniform(0.0, 0.1) 

# ==========================================
# 2. HARR Core Implementation (è«–æ–‡ã®å®Ÿè£…)
# ==========================================

class HARRRetriever(nn.Module):
    def __init__(self, query_encoder, temperature=1.0):
        super().__init__()
        self.query_encoder = query_encoder
        self.temperature = temperature

    def forward(self, state_input_ids, candidate_doc_embs):
        """
        ã‚¯ã‚¨ãƒª(State)ã¨å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        """
        state_emb = self.query_encoder(state_input_ids)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ãŸã‚ã®æ­£è¦åŒ–
        state_emb = F.normalize(state_emb, p=2, dim=1)
        candidate_doc_embs = F.normalize(candidate_doc_embs, p=2, dim=2)

        # å†…ç©è¨ˆç®— [Batch, 1, Dim] x [Batch, Pool, Dim]^T -> [Batch, 1, Pool]
        scores = torch.bmm(
            candidate_doc_embs,
            state_emb.unsqueeze(2)
        ).squeeze(2) 
        return scores

    def sample_documents(self, scores, k_retrieve):
        """
        Plackett-Luce Sampling: ç¢ºç‡ã«åŸºã¥ã„ã¦kå€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’éå¾©å…ƒæŠ½å‡º
        """
        batch_size, pool_size = scores.shape
        selected_indices = []
        action_log_probs = []
        
        # é¸æŠæ¸ˆã¿ãƒã‚¹ã‚¯
        mask = torch.zeros_like(scores, dtype=torch.bool)
        logits = scores / self.temperature
        
        for _ in range(k_retrieve):
            # é¸æŠæ¸ˆã¿ã®ã‚¹ã‚³ã‚¢ã‚’ -inf ã«ã—ã¦é¸ã°ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹
            masked_logits = logits.clone()
            masked_logits[mask] = float('-inf')
            
            # ç¢ºç‡åˆ†å¸ƒä½œæˆ
            probs = F.softmax(masked_logits, dim=-1)
            dist = torch.distributions.Categorical(probs)
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
            action = dist.sample() # [Batch]
            log_prob = dist.log_prob(action)
            
            selected_indices.append(action)
            action_log_probs.append(log_prob)
            
            # ãƒã‚¹ã‚¯æ›´æ–° (In-placeæ“ä½œã‚’é¿ã‘ã‚‹ãŸã‚è«–ç†å’Œã‚’ä½¿ç”¨)
            step_mask = torch.zeros_like(mask).scatter(1, action.unsqueeze(1), True)
            mask = mask | step_mask
            
        selected_indices = torch.stack(selected_indices, dim=1)
        # è»Œè·¡å…¨ä½“ã®å¯¾æ•°ç¢ºç‡ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã®å’Œ
        total_log_probs = torch.stack(action_log_probs, dim=1).sum(dim=1)
        
        return selected_indices, total_log_probs

def compute_grpo_loss(current_log_probs, old_log_probs, rewards, clip_epsilon=0.2):
    """
    GRPO Lossé–¢æ•°ã®è¨ˆç®—
    """
    # 1. Advantageã®è¨ˆç®— (ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®æ­£è¦åŒ–)
    mean_r = rewards.mean()
    std_r = rewards.std() + 1e-8
    advantages = (rewards - mean_r) / std_r
    advantages = advantages.detach() # å‹¾é…ã‚’åˆ‡ã‚‹
    
    # 2. Importance Sampling Ratio
    ratio = torch.exp(current_log_probs - old_log_probs)
    
    # 3. Clipped Surrogate Objective
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
    
    # æœ€å¤§åŒ–ã—ãŸã„ã®ã§ãƒã‚¤ãƒŠã‚¹ã‚’ã‹ã‘ã¦æœ€å°åŒ–å•é¡Œã«ã™ã‚‹
    loss = -torch.min(surr1, surr2).mean()
    return loss

# ==========================================
# 3. Main Training Loop (å®Ÿè¡Œãƒ‡ãƒ¢)
# ==========================================

def run_harr_demo():
    print("ğŸš€ HARR Training Demo Started...")
    torch.manual_seed(42)
    np.random.seed(42)

    # --- Hyperparameters ---
    vocab_size = 100
    embed_dim = 32
    pool_size = 10     # å€™è£œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
    k_retrieve = 3     # å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
    group_size = 8     # GRPOã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º (1ã‚¯ã‚¨ãƒªã‚ãŸã‚Šã®è©¦è¡Œå›æ•°)
    steps = 50         # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°
    learning_rate = 0.05
    
    # --- Setup ---
    encoder = MockQueryEncoder(vocab_size, embed_dim)
    retriever = HARRRetriever(encoder, temperature=1.0)
    optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ (å›ºå®š)
    doc_embs = torch.randn(pool_size, embed_dim)
    doc_embs = F.normalize(doc_embs, p=2, dim=1)
    
    # Query 0 ã®æ­£è§£ã¯ Doc 0 ã¨ã™ã‚‹
    gt_map = {0: 0} 
    env = MockEnvironment(doc_embs, gt_map)
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªãƒ‡ãƒ¼ã‚¿
    query_input_ids = torch.randint(0, vocab_size, (1, 10)) 
    
    print(f"ğŸ¯ Objective: Queryã«å¯¾ã—ã€æ­£è§£ã® 'Document #0' ã‚’Retrievalã•ã›ã‚‹")
    print("-" * 40)

    for step in range(1, steps + 1):
        optimizer.zero_grad()
        
        # ãƒãƒƒãƒä½œæˆ: åŒã˜ã‚¯ã‚¨ãƒªã‚’Group Sizeåˆ†è¤‡è£½ã—ã¦ä¸¦åˆ—è©¦è¡Œã•ã›ã‚‹
        batch_input_ids = query_input_ids.repeat(group_size, 1)
        batch_candidate_embs = doc_embs.unsqueeze(0).repeat(group_size, 1, 1)
        
        # --- 1. Experience Collection (Old Policy) ---
        # ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã‚‹
        with torch.no_grad():
            scores = retriever(batch_input_ids, batch_candidate_embs)
            selected_indices, old_log_probs = retriever.sample_documents(scores, k_retrieve)
        
        # --- 2. Reward Calculation ---
        # é¸ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¯¾ã—ã¦å ±é…¬ã‚’ã‚‚ã‚‰ã†
        rewards = []
        for i in range(group_size):
            indices = selected_indices[i].tolist()
            r = env.get_reward(0, indices) # Query ID 0
            rewards.append(r)
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
        
        # --- 3. Policy Update (New Policy) ---
        # å‹¾é…è¨ˆç®—ã®ãŸã‚ã«å†åº¦Forwardè¨ˆç®—
        new_scores = retriever(batch_input_ids, batch_candidate_embs)
        new_logits = new_scores / retriever.temperature
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã¨åŒã˜ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºç‡ã‚’ã€å‹¾é…ä»˜ãã§å†è¨ˆç®—ã™ã‚‹
        new_log_probs_list = []
        mask = torch.zeros_like(new_scores, dtype=torch.bool)
        
        for k in range(k_retrieve):
            actions_at_step = selected_indices[:, k] # Rolloutã§é¸ã‚“ã ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            
            # Masking
            masked_logits = new_logits.clone()
            masked_logits[mask] = float('-inf')
            
            probs = F.softmax(masked_logits, dim=-1)
            dist = torch.distributions.Categorical(probs)
            
            # é¸ã‚“ã ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å¯¾æ•°ç¢ºç‡
            log_prob = dist.log_prob(actions_at_step)
            new_log_probs_list.append(log_prob)
            
            # ãƒã‚¹ã‚¯æ›´æ–° (è«–ç†å’Œ)
            step_mask = torch.zeros_like(mask).scatter(1, actions_at_step.unsqueeze(1), True)
            mask = mask | step_mask
            
        new_log_probs = torch.stack(new_log_probs_list, dim=1).sum(dim=1)
        
        # Lossè¨ˆç®— & ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        loss = compute_grpo_loss(new_log_probs, old_log_probs, rewards_tensor)
        loss.backward()
        optimizer.step()
        
        # ãƒ­ã‚°å‡ºåŠ›
        if step % 10 == 0:
            avg_reward = rewards_tensor.mean().item()
            # Doc #0 (æ­£è§£) ãŒå«ã¾ã‚Œã¦ã„ãŸå‰²åˆ (Recall@K)
            success_rate = (selected_indices == 0).any(dim=1).float().mean().item()
            print(f"Step {step:02d} | Loss: {loss.item():.4f} | Avg Reward: {avg_reward:.2f} | Success Rate: {success_rate:.0%}")

    print("-" * 40)
    print("ğŸ‰ Training Finished!")

if __name__ == "__main__":
    run_harr_demo()