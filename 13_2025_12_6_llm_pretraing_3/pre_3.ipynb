{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj70dN_PpiEB",
        "outputId": "c2ce6b33-acab-45e3-b024-048fb85a02ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 0. „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´„Å®„Ç§„É≥„Éù„Éº„Éà\n",
        "# ==========================================\n",
        "!pip install datasets tqdm tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aTeuLbODpj2h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "import tqdm\n",
        "import math\n",
        "import time\n",
        "import gc\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xknuafx1pmKL",
        "outputId": "cb6b3b3a-c02a-4909-ae1f-9c506e3561cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# „Éá„Éê„Ç§„ÇπË®≠ÂÆö (GPUÊé®Â•®)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cpu':\n",
        "    print(\"‚ö†Ô∏è Ê≥®ÊÑè: GPU„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇÂ≠¶Áøí„ÅØÈùûÂ∏∏„Å´ÈÅÖ„Åè„Å™„Çä„Åæ„Åô„ÄÇ„É©„É≥„Çø„Ç§„É†„ÅÆË®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "6503b51e254c42428ad0f500ac64796b",
            "e3391d204d5449fc83cca5e2265e8343",
            "4ab3ee63f769467fa609f7e531753b95",
            "c5e05e0a6a4741019578c5df9227e255",
            "9d37867cbcd448c9abf208f34e5e0282",
            "6d571596c9a64c9ab6cab47bda049dca",
            "3bbbb6fa9d464af1b42e36ef51859033",
            "3c4ac0d384f846569b66d8fddf275c4b",
            "2c24b16479b2446e9031b2c30a57f3ef",
            "2fe0653d9fa84cf9acab89eaa991cc27",
            "83b69526b98b4830a23a47eebd577e57"
          ]
        },
        "id": "MsEpD-XSpoaN",
        "outputId": "08bc0595-4775-4d18-eeaf-93ef8b903c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ‰∏≠...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6503b51e254c42428ad0f500ac64796b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‰øùÂ≠ò„ÇíÈñãÂßã„Åó„Åæ„Åô: wiki_ja_subset.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11724it [00:09, 1264.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàêÂÆå‰∫ÜÔºÅ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàê (WikiÊäΩÂá∫)\n",
        "# ==========================================\n",
        "CORPUS_FILE = \"wiki_ja_subset.txt\"\n",
        "\n",
        "if not os.path.exists(CORPUS_FILE):\n",
        "    print(\"„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ‰∏≠...\")\n",
        "    dataset = load_dataset(\"izumi-lab/wikipedia-ja-20230720\", split=\"train\", streaming=True)\n",
        "\n",
        "    print(f\"‰øùÂ≠ò„ÇíÈñãÂßã„Åó„Åæ„Åô: {CORPUS_FILE}\")\n",
        "    with open(CORPUS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        count = 0\n",
        "        for data in tqdm.tqdm(dataset):\n",
        "            text = data[\"text\"].replace(\"\\n\", \"\")\n",
        "            if len(text) > 100: # Áü≠„Åô„Åé„ÇãË®ò‰∫ã„ÅØÈô§Â§ñ\n",
        "                f.write(text + \"\\n\")\n",
        "                count += 1\n",
        "            if count >= 10000: # 1‰∏á‰ª∂„Åß„Çπ„Éà„ÉÉ„Éó\n",
        "                break\n",
        "    print(\"‚úÖ „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàêÂÆå‰∫ÜÔºÅ\")\n",
        "else:\n",
        "    print(f\"‚úÖ „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØÊó¢„Å´Â≠òÂú®„Åó„Åæ„Åô: {CORPUS_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8gix_ivps5_",
        "outputId": "ccb41955-e88e-4713-cb09-0cd6e1d91f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÇíÂ≠¶Áøí‰∏≠...\n",
            "„Éà„Éº„ÇØ„Éä„Ç§„Ç∂Â≠¶ÁøíÂÆå‰∫Ü„ÄÇPad ID: 1\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 2. „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÅÆÂ≠¶Áøí (BPE)\n",
        "# ==========================================\n",
        "TOKENIZER_FILE = \"custom_tokenizer.json\"\n",
        "VOCAB_SIZE = 32000\n",
        "\n",
        "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÅÆÊßãÁØâ\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<|endoftext|>\", \"<|pad|>\"],\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(\"„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÇíÂ≠¶Áøí‰∏≠...\")\n",
        "tokenizer.train([CORPUS_FILE], trainer)\n",
        "tokenizer.save(TOKENIZER_FILE)\n",
        "pad_token_id = tokenizer.token_to_id(\"<|pad|>\")\n",
        "print(f\"„Éà„Éº„ÇØ„Éä„Ç§„Ç∂Â≠¶ÁøíÂÆå‰∫Ü„ÄÇPad ID: {pad_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LNsLnBH1pv_W"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇØ„É©„Çπ (ÁúÅ„É°„É¢„É™Áâà)\n",
        "# ==========================================\n",
        "class LazyLLMPretrainDataset(Dataset):\n",
        "    def __init__(self, txt_file, tokenizer, max_length=256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.txt_file = txt_file\n",
        "        self.offsets = []\n",
        "\n",
        "        print(\"„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàê‰∏≠ („Éï„Ç°„Ç§„É´‰ΩçÁΩÆ„ÅÆË®òÈå≤)...\")\n",
        "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            offset = 0\n",
        "            for line in f:\n",
        "                self.offsets.append(offset)\n",
        "                offset += len(line.encode('utf-8')) # „Éê„Ç§„ÉàÊï∞„Åß„Ç™„Éï„Çª„ÉÉ„Éà„ÇíÈÄ≤„ÇÅ„Çã\n",
        "        print(f\"„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàêÂÆå‰∫Ü: {len(self.offsets)} Ë°å\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.offsets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ÂøÖË¶Å„Å™Ë°å„Å†„Åë„Çí„Éá„Ç£„Çπ„ÇØ„Åã„ÇâË™≠„ÅøËæº„ÇÄ (RAMÁØÄÁ¥Ñ)\n",
        "        with open(self.txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            f.seek(self.offsets[idx])\n",
        "            line = f.readline()\n",
        "\n",
        "        # „Éà„Éº„ÇØ„Éä„Ç§„Ç∫\n",
        "        encoded = self.tokenizer.encode(line)\n",
        "        ids = encoded.ids\n",
        "\n",
        "        # Èï∑„ÅïË™øÊï¥ (Truncate or Pad)\n",
        "        if len(ids) > self.max_length:\n",
        "            ids = ids[:self.max_length]\n",
        "        else:\n",
        "            # Áü≠„ÅÑÂ†¥Âêà„ÅØ„Éë„Éá„Ç£„É≥„Ç∞ (<|pad|> ID„Çí‰ΩøÁî®)\n",
        "            ids = ids + [pad_token_id] * (self.max_length - len(ids))\n",
        "\n",
        "        x = torch.tensor(ids, dtype=torch.long)\n",
        "        y = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cpTeTRv4p03q"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 4. „É¢„Éá„É´ÂÆöÁæ© (Llama Architecture)\n",
        "# ==========================================\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 512          # Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉ\n",
        "    n_layers: int = 6       # „É¨„Ç§„É§„ÉºÊï∞ (8->6 „Å´ËªΩÈáèÂåñ)\n",
        "    n_heads: int = 8        # „Éò„ÉÉ„ÉâÊï∞\n",
        "    vocab_size: int = 32000\n",
        "    multiple_of: int = 256\n",
        "    max_seq_len: int = 256  # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÈï∑ („Åì„Åì„Åå„É°„É¢„É™„Å´Âäπ„Åè)\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    @property\n",
        "    def head_dim(self):\n",
        "        return self.dim // self.n_heads\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._norm(x.float()).type_as(x) * self.weight\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_cis\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = freqs_cis[:xq.shape[1]].view(1, xq.shape[1], 1, -1)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.head_dim = args.head_dim\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq = self.wq(x).view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xk = self.wk(x).view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xv = self.wv(x).view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "\n",
        "        output = F.scaled_dot_product_attention(\n",
        "            xq.transpose(1, 2), xk.transpose(1, 2), xv.transpose(1, 2), is_causal=True\n",
        "        )\n",
        "        return self.wo(output.transpose(1, 2).contiguous().view(bsz, seqlen, -1))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * args.dim\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
        "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.attention_norm = RMSNorm(args.dim)\n",
        "        self.attention = Attention(args)\n",
        "        self.ffn_norm = RMSNorm(args.dim)\n",
        "        self.feed_forward = FeedForward(args)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        h = x + self.attention(self.attention_norm(x), freqs_cis)\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.n_layers)])\n",
        "        self.norm = RMSNorm(args.dim)\n",
        "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
        "        self.freqs_cis = precompute_freqs_cis(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        bsz, seqlen = idx.shape\n",
        "        x = self.tok_embeddings(idx)\n",
        "        freqs_cis = self.freqs_cis[:seqlen].to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, freqs_cis)\n",
        "        x = self.norm(x)\n",
        "        return self.output(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vL4TtvHp00M",
        "outputId": "cce16ee1-74b3-4d29-cfbb-49a7e66d26f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàê‰∏≠ („Éï„Ç°„Ç§„É´‰ΩçÁΩÆ„ÅÆË®òÈå≤)...\n",
            "„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ‰ΩúÊàêÂÆå‰∫Ü: 10000 Ë°å\n",
            "Model parameters: 53.2M\n",
            "\n",
            "üöÄ Â≠¶Áøí„ÇíÈñãÂßã„Åó„Åæ„Åô...\n",
            "Step 0/1250 | Loss: 10.5534 | LR: 0.000000 | Time: 0.9s\n",
            "Step 100/1250 | Loss: 9.2830 | LR: 0.000242 | Time: 13.7s\n",
            "Step 200/1250 | Loss: 8.5183 | LR: 0.000297 | Time: 25.6s\n",
            "Step 300/1250 | Loss: 8.0674 | LR: 0.000282 | Time: 37.4s\n",
            "Step 400/1250 | Loss: 7.9703 | LR: 0.000258 | Time: 49.8s\n",
            "Step 500/1250 | Loss: 8.0296 | LR: 0.000225 | Time: 62.0s\n",
            "Step 600/1250 | Loss: 7.7718 | LR: 0.000186 | Time: 74.4s\n",
            "Step 700/1250 | Loss: 7.7683 | LR: 0.000144 | Time: 85.9s\n",
            "Step 800/1250 | Loss: 7.6845 | LR: 0.000103 | Time: 98.3s\n",
            "Step 900/1250 | Loss: 7.7206 | LR: 0.000066 | Time: 110.5s\n",
            "Step 1000/1250 | Loss: 7.5657 | LR: 0.000035 | Time: 122.8s\n",
            "Step 1100/1250 | Loss: 7.6166 | LR: 0.000030 | Time: 135.3s\n",
            "Step 1200/1250 | Loss: 7.3080 | LR: 0.000030 | Time: 147.7s\n",
            "\n",
            "üéâ Â≠¶ÁøíÂÆå‰∫ÜÔºÅ\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 5. Â≠¶ÁøíË®≠ÂÆö & „É´„Éº„Éó (ÊúÄÈÅ©ÂåñÊ∏à„Åø)\n",
        "# ==========================================\n",
        "# ËªΩÈáèË®≠ÂÆö\n",
        "BATCH_SIZE = 8           # VRAMÁØÄÁ¥Ñ„ÅÆ„Åü„ÇÅÂ∞è„Åï„Åè\n",
        "ACCUMULATION_STEPS = 4   # 8 * 4 = 32 (ÂÆüË≥™„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫)\n",
        "EPOCHS = 1               # „Éá„É¢„ÅÆ„Åü„ÇÅ1„Ç®„Éù„ÉÉ„ÇØ\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "# „Éá„Éº„Çø„Çª„ÉÉ„Éà & „É≠„Éº„ÉÄ„Éº\n",
        "dataset = LazyLLMPretrainDataset(CORPUS_FILE, tokenizer, max_length=MAX_LENGTH)\n",
        "# num_workers=0 „ÅåÈáçË¶Å (Colab„ÅÆRAM„ÇØ„É©„ÉÉ„Ç∑„É•Èò≤Ê≠¢)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "# „É¢„Éá„É´ÂàùÊúüÂåñ\n",
        "args = ModelArgs(\n",
        "    dim=512, n_layers=6, n_heads=8,\n",
        "    vocab_size=VOCAB_SIZE, max_seq_len=MAX_LENGTH\n",
        ")\n",
        "model = Transformer(args).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "total_steps = len(dataloader) * EPOCHS // ACCUMULATION_STEPS\n",
        "\n",
        "def get_scheduler(optimizer, warmup_steps, training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, training_steps - warmup_steps))\n",
        "        return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_scheduler(optimizer, int(total_steps * 0.1), total_steps)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id) # „Éë„Éá„Ç£„É≥„Ç∞„ÅØLossË®àÁÆó„Åã„ÇâÈô§Â§ñ\n",
        "scaler = GradScaler() # Mixed PrecisionÁî®\n",
        "\n",
        "print(\"\\nüöÄ Â≠¶Áøí„ÇíÈñãÂßã„Åó„Åæ„Åô...\")\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "\n",
        "start_time = time.time()\n",
        "total_loss = 0\n",
        "\n",
        "for step, (x, y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward (Mixed Precision)\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.float16): # T4„Å™„Çâfloat16Êé®Â•®\n",
        "        logits = model(x)\n",
        "\n",
        "        # Shift for prediction\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = y[..., 1:].contiguous()\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, args.vocab_size), shift_labels.view(-1))\n",
        "\n",
        "        # 2. ÂãæÈÖçËìÑÁ©ç„ÅÆ„Åü„ÇÅ„ÅÆLossÂâ≤„ÇäÁÆó\n",
        "        loss = loss / ACCUMULATION_STEPS\n",
        "\n",
        "    # 3. Backward\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # 4. Update (‰∏ÄÂÆö„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´ÂÆüË°å)\n",
        "    if (step + 1) % ACCUMULATION_STEPS == 0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True) # „É°„É¢„É™Ëß£Êîæ\n",
        "\n",
        "    total_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "    # „É≠„Ç∞Ë°®Á§∫\n",
        "    if step % 100 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Step {step}/{len(dataloader)} | Loss: {loss.item() * ACCUMULATION_STEPS:.4f} | LR: {current_lr:.6f} | Time: {elapsed:.1f}s\")\n",
        "\n",
        "        # „É°„É¢„É™ÊéÉÈô§\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nüéâ Â≠¶ÁøíÂÆå‰∫ÜÔºÅ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTodIGIhqB5s",
        "outputId": "d75e4adb-0b84-4bb2-ea88-9837bde10d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ÁîüÊàêÁµêÊûú ---\n",
            "‰∫∫Â∑•Áü•ËÉΩ„ÅØ„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„ÄÅÊó•Êú¨\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 6. ÁîüÊàê„ÉÜ„Çπ„Éà\n",
        "# ==========================================\n",
        "def generate_text(model, start_text, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    ids = tokenizer.encode(start_text).ids\n",
        "    input_ids = torch.tensor([ids], device=device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "            # ÁµÇ‰∫Ü„Çø„Ç∞„ÅåÂá∫„Åü„ÇâÊ≠¢„ÇÅ„Çã\n",
        "            if next_token.item() == tokenizer.token_to_id(\"<|endoftext|>\"):\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(input_ids[0].tolist())\n",
        "\n",
        "print(\"\\n--- ÁîüÊàêÁµêÊûú ---\")\n",
        "print(generate_text(model, \"‰∫∫Â∑•Áü•ËÉΩ„ÅØ\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6503b51e254c42428ad0f500ac64796b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3391d204d5449fc83cca5e2265e8343",
              "IPY_MODEL_4ab3ee63f769467fa609f7e531753b95",
              "IPY_MODEL_c5e05e0a6a4741019578c5df9227e255"
            ],
            "layout": "IPY_MODEL_9d37867cbcd448c9abf208f34e5e0282"
          }
        },
        "e3391d204d5449fc83cca5e2265e8343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d571596c9a64c9ab6cab47bda049dca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3bbbb6fa9d464af1b42e36ef51859033",
            "value": "README.md:‚Äá100%"
          }
        },
        "4ab3ee63f769467fa609f7e531753b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c4ac0d384f846569b66d8fddf275c4b",
            "max": 480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c24b16479b2446e9031b2c30a57f3ef",
            "value": 480
          }
        },
        "c5e05e0a6a4741019578c5df9227e255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe0653d9fa84cf9acab89eaa991cc27",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83b69526b98b4830a23a47eebd577e57",
            "value": "‚Äá480/480‚Äá[00:00&lt;00:00,‚Äá56.5kB/s]"
          }
        },
        "9d37867cbcd448c9abf208f34e5e0282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d571596c9a64c9ab6cab47bda049dca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bbbb6fa9d464af1b42e36ef51859033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c4ac0d384f846569b66d8fddf275c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c24b16479b2446e9031b2c30a57f3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fe0653d9fa84cf9acab89eaa991cc27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b69526b98b4830a23a47eebd577e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}