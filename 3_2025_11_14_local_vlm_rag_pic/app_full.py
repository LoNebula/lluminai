import time
import tempfile
from pathlib import Path
import subprocess

from fastapi import FastAPI, File, UploadFile
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from ollama_client import analyze_image_with_ollama
from rag_pipeline import index_markdown, answer_with_context

app = FastAPI(title="Multimodal RAG Pipeline")

# CORS (å¿…è¦ã«å¿œã˜ã¦èª¿æ•´)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ« (index.html, JS, CSS)
app.mount("/static", StaticFiles(directory="static"), name="static")


def check_ollama_mode():
    """
    Ollama ãŒ GPU / CPU ã®ã©ã¡ã‚‰ã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ã‚’ã–ã£ãã‚Šè¡¨ç¤ºï¼ˆä»»æ„ï¼‰
    Windows ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ä¾‹ C:\\ollama\\ollama.exe ã‚’æƒ³å®š
    """
    try:
        OLLAMA_CMD = r"C:\ollama\ollama.exe"

        result = subprocess.run(
            [OLLAMA_CMD, "list", "hardware"],
            capture_output=True,
            text=True,
        )
        out = result.stdout.lower()

        if "nvidia" in out or "cuda" in out:
            return "GPU"
        return "CPU"
    except Exception as e:
        print("âš ï¸ GPU/CPU åˆ¤å®šã«å¤±æ•—:", e)
        return "ä¸æ˜"


@app.on_event("startup")
async def startup_event():
    print("ğŸš€ FastAPI started")
    mode = check_ollama_mode()
    print(f"ğŸ’¡ Ollama is using: {mode} mode")


@app.get("/", response_class=HTMLResponse)
async def index():
    """
    ãƒ¡ã‚¤ãƒ³ç”»é¢ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶UIï¼‰
    """
    return Path("static/index.html").read_text(encoding="utf-8")


# ========== VLM: ç”»åƒ â†’ Markdown â†’ RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ==========

@app.post("/api/analyze")
async def analyze(file: UploadFile = File(...)):
    """
    ç”»åƒã‚’è§£æã—ã¦ Markdown ã‚’è¿”ã—ã¤ã¤ã€ãã®ã¾ã¾ RAG ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤:
    - markdown: ç”Ÿæˆã•ã‚ŒãŸMarkdown
    - exec_time_sec: VLMå®Ÿè¡Œæ™‚é–“
    - source_id: ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
    - num_chunks: ä¿å­˜ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°
    """
    start_time = time.time()

    suffix = Path(file.filename).suffix or ".png"
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp_path = Path(tmp.name)
        tmp.write(await file.read())

    try:
        # 1. VLMã§Markdownç”Ÿæˆ
        md = analyze_image_with_ollama(tmp_path)

        # 2. ãã®ã¾ã¾RAGã«æŠ•å…¥ï¼ˆsource_idã«ã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ã†ï¼‰
        index_info = index_markdown(md, source_id=file.filename)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
    finally:
        tmp_path.unlink(missing_ok=True)

    exec_time = time.time() - start_time
    print(f"â±ï¸ VLM+Index Execution time: {exec_time:.2f} sec")

    return {
        "markdown": md,
        "exec_time_sec": exec_time,
        "source_id": index_info["source_id"],
        "num_chunks": index_info["num_chunks"],
    }


# ========== RAG: è³ªå• â†’ å›ç­” ==========

class QueryBody(BaseModel):
    question: str
    top_k: int = 5


@app.post("/api/query")
async def query_rag(body: QueryBody):
    """
    RAG ã«è³ªå•ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    - body.question: è³ªå•æ–‡ï¼ˆæ—¥æœ¬èªã§OKï¼‰
    - body.top_k   : å–å¾—ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ï¼‰

    æˆ»ã‚Šå€¤:
    - answer: LLMã«ã‚ˆã‚‹æœ€çµ‚å›ç­”
    - contexts: å‚ç…§ã—ãŸãƒãƒ£ãƒ³ã‚¯ï¼ˆdocuments, metadatasï¼‰
    """
    try:
        result = answer_with_context(body.question, k=body.top_k)
        return result
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# ========== é–‹ç™ºç”¨: uvicorn ã‹ã‚‰ç›´æ¥èµ·å‹•ã™ã‚‹å ´åˆ ==========

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app_full:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
    )
