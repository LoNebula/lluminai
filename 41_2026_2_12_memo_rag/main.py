import os
from typing import List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import dotenv

# ==========================================
# 0. è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
# ==========================================

dotenv.load_dotenv()
# ç’°å¢ƒå¤‰æ•°åãŒç•°ãªã‚‹å ´åˆã¯é©å®œä¿®æ­£ã—ã¦ãã ã•ã„
key = os.getenv("OPENAI_API_KEY_LLUMINAI") 
os.environ["OPENAI_API_KEY"] = key

# ç‰©èªãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
with open("./story.txt", "r", encoding="utf-8") as f:
    STORY_TEXT = f.read()

# ==========================================
# 1. MemoRAG ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…ï¼ˆLangChainå¯¾å¿œç‰ˆï¼‰
# ==========================================

class ConceptualMemoRAG:
    def __init__(self, llm_fast, llm_smart, retriever):
        self.memory_model = llm_fast
        self.generator_model = llm_smart
        self.retriever = retriever
        self.global_memory = ""

    def memorize(self, long_context: str):
        """Step 1: Memory Formation"""
        print("\nğŸ§  [Step 1] Forming Global Memory...")
        
        # ç‰©èªç”¨ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å°‘ã—èª¿æ•´ï¼ˆç™»å ´äººç‰©ã‚„ãƒ—ãƒ­ãƒƒãƒˆé‡è¦–ï¼‰
        prompt = (
            f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã„ç‰©èªã§ã™ã€‚\n"
            f"å¾Œã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ã€ã“ã®ç‰©èªã®ã€Œã‚ã‚‰ã™ã˜ã€ã€Œä¸»è¦ãªç™»å ´äººç‰©ã®é–¢ä¿‚æ€§ã€ã€Œçµæœ«ã«è‡³ã‚‹é‡è¦ãªä¼ç·šã€ã‚’\n"
            f"è©³ç´°ã‹ã¤ç°¡æ½”ã«è¦ç´„ã—ã¦è¨˜æ†¶ãƒ¡ãƒ¢ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
            f"--- ç‰©èª ---\n{long_context[:15000]}...\n-------------------" # ãƒˆãƒ¼ã‚¯ãƒ³æº¢ã‚Œé˜²æ­¢ã®ãŸã‚truncate
        )
        
        response = self.memory_model.invoke(prompt)
        self.global_memory = response.content
        print(f"âœ… Memory Created (Length: {len(self.global_memory)} chars)")
        print(f"   (Content Preview: {self.global_memory[:50]}...)")

    def recall_clues(self, query: str) -> List[str]:
        """Step 2: Clue Generation"""
        print(f"\nğŸ¤” [Step 2] Thinking about clues for: '{query}'")
        
        prompt = (
            f"ã‚ãªãŸã¯ç‰©èªå…¨ä½“ã®ã€Œã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã€ã‚’æŒã£ã¦ã„ã¾ã™ã€‚\n"
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«ã€ç‰©èªã®ã€Œã©ã®å ´é¢ã€ã‚’æ¢ã›ã°ã‚ˆã„ã‹ã€å…·ä½“çš„ãªã€Œæ‰‹ãŒã‹ã‚Šï¼ˆCluesï¼‰ã€ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
            f"æ‰‹ãŒã‹ã‚Šã¯æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ã‚ã‚Œã¾ã™ã€‚\n\n"
            f"ã€ãƒ¡ãƒ¢ãƒªã€‘: {self.global_memory}\n"
            f"ã€è³ªå•ã€‘: {query}\n\n"
            f"æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ãˆã‚‹çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ç®‡æ¡æ›¸ãã§3ã¤å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        )
        
        response = self.memory_model.invoke(prompt)
        clues = [line.strip().replace("- ", "").replace("ãƒ»", "") for line in response.content.split('\n') if line.strip()]
        
        print(f"ğŸ’¡ Generated Clues: {clues}")
        return clues

    def retrieve_evidence(self, clues: List[str]) -> str:
        """Step 3: Retrieval"""
        print("\nğŸ” [Step 3] Retrieving evidence based on clues...")
        
        aggregated_evidence = set()
        for clue in clues:
            # â˜…ã“ã“ã‚’ä¿®æ­£ã—ã¾ã—ãŸ (get_relevant_documents -> invoke)
            docs = self.retriever.invoke(clue)
            for d in docs:
                aggregated_evidence.add(d.page_content)
        
        final_evidence = "\n\n".join(list(aggregated_evidence))
        print(f"ğŸ“š Retrieved {len(aggregated_evidence)} chunks of evidence.")
        return final_evidence

    def generate_response(self, query: str, evidence: str) -> str:
        """Step 4: Final Generation"""
        print("\nğŸ“ [Step 4] Generating final answer...")
        
        prompt = (
            f"ä»¥ä¸‹ã®ã€æ¤œç´¢ã•ã‚ŒãŸè¨¼æ‹ ã€‘ã«åŸºã¥ã„ã¦ã€è³ªå•ã«æ„Ÿæƒ…è±Šã‹ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            f"ç‰©èªã®æ–‡è„ˆã‚’åæ˜ ã•ã›ã¦ãã ã•ã„ã€‚\n\n"
            f"ã€è³ªå•ã€‘: {query}\n\n"
            f"ã€æ¤œç´¢ã•ã‚ŒãŸè¨¼æ‹ ã€‘:\n{evidence}"
        )
        
        response = self.generator_model.invoke(prompt)
        return response.content

# ==========================================
# 2. å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ==========================================

def main():
    print("âš™ï¸ Initializing Vector Database...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    chunks = [p.strip() for p in STORY_TEXT.split("\n\n") if p.strip()]
    documents = [Document(page_content=c) for c in chunks]
    
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(documents, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    llm_fast = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    llm_smart = ChatOpenAI(model="gpt-4o", temperature=0)

    rag = ConceptualMemoRAG(llm_fast=llm_fast, llm_smart=llm_smart, retriever=retriever)

    # 1. è¨˜æ†¶å½¢æˆ
    rag.memorize(STORY_TEXT)

    # 2. è³ªå•
    query = "è—¤åŸå¥ä¸€ãŒ15å¹´å‰ã«ç™½çŸ³å¹¸å­ã®å‰ã‹ã‚‰å§¿ã‚’æ¶ˆã—ãŸæœ¬å½“ã®ç†ç”±ã¯ä½•ã§ã™ã‹ï¼Ÿã¾ãŸã€äºŒäººã®çµæœ«ã¯ã©ã†ãªã‚Šã¾ã—ãŸã‹ï¼Ÿ"
    
    # 3. å®Ÿè¡Œ
    clues = rag.recall_clues(query)
    evidence = rag.retrieve_evidence(clues)
    final_answer = rag.generate_response(query, evidence)

    print("\n" + "="*50)
    print("ğŸ¤– Final Answer:")
    print("="*50)
    print(final_answer)

if __name__ == "__main__":
    main()